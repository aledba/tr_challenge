{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "032a2602",
   "metadata": {},
   "source": [
    "# 03 - Model Evaluation: Checkpoint Analysis\n",
    "\n",
    "**Objective**: Load and analyze the trained Legal-Longformer checkpoint.\n",
    "\n",
    "This notebook evaluates the model saved during training, examining:\n",
    "- Overall metrics (F1, Precision, Recall)\n",
    "- Per-class performance\n",
    "- Confusion analysis\n",
    "- Comparison with TF-IDF baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd5946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data_loader import DataLoader\n",
    "from src.model_trainer import DataPreparer\n",
    "from src.model_evaluator import MultiLabelEvaluator\n",
    "from src.bert_trainer import (\n",
    "    DeviceManager, \n",
    "    HybridLegalClassifier,\n",
    "    LegalLongformerTrainer,\n",
    ")\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e42613",
   "metadata": {},
   "source": [
    "## 1. Load Data and Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb13e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (same as training)\n",
    "loader = DataLoader('../data/TRDataChallenge2023.txt')\n",
    "preparer = DataPreparer(loader, min_label_count=50, random_state=42)\n",
    "data = preparer.prepare(max_features=10000, ngram_range=(1, 2))\n",
    "\n",
    "print(data.summary())\n",
    "print(f\"\\nLabels: {len(data.label_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e018f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available model files\n",
    "import json\n",
    "\n",
    "model_path = Path('../outputs/legal_longformer_best.pt')\n",
    "history_path = Path('../outputs/legal_longformer_best.history.json')\n",
    "\n",
    "print(\"Model files:\")\n",
    "history = None\n",
    "\n",
    "if model_path.exists():\n",
    "    size_mb = model_path.stat().st_size / 1e6\n",
    "    print(f\"  {model_path.name}: {size_mb:.1f} MB ✓\")\n",
    "else:\n",
    "    print(f\"  {model_path.name}: NOT FOUND ❌\")\n",
    "\n",
    "if history_path.exists():\n",
    "    print(f\"  {history_path.name} ✓\")\n",
    "    with open(history_path) as f:\n",
    "        history = json.load(f)\n",
    "    print(f\"\\nTraining History:\")\n",
    "    print(f\"  Best F1:    {history['best_f1']:.4f}\")\n",
    "    print(f\"  Best epoch: {history['best_epoch']}\")\n",
    "else:\n",
    "    print(f\"  {history_path.name}: NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb2600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training history (from saved JSON)\n",
    "if history:\n",
    "    print(\"Training progression:\")\n",
    "    print(f\"{'Epoch':<8} {'Train Loss':<12} {'Val Loss':<12} {'Val F1 Micro':<14} {'Val F1 Macro':<14}\")\n",
    "    print(\"-\" * 60)\n",
    "    for i in range(len(history['train_losses'])):\n",
    "        best_marker = \" ← best\" if (i + 1) == history['best_epoch'] else \"\"\n",
    "        print(f\"{i+1:<8} {history['train_losses'][i]:<12.4f} {history['val_losses'][i]:<12.4f} \"\n",
    "              f\"{history['val_f1_micro'][i]:<14.4f} {history['val_f1_macro'][i]:<14.4f}{best_marker}\")\n",
    "else:\n",
    "    print(\"No training history available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb14fbf",
   "metadata": {},
   "source": [
    "## 2. Load Model from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a46240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hybrid classifier and load best model\n",
    "hybrid_classifier = HybridLegalClassifier(\n",
    "    num_labels=len(data.label_names),\n",
    "    cache_dir='../outputs/summaries',\n",
    "    device='auto',\n",
    ")\n",
    "\n",
    "# Load the best model (saved after training completed)\n",
    "hybrid_classifier.load('../outputs/legal_longformer_best.pt')\n",
    "\n",
    "if history:\n",
    "    best_epoch = history['best_epoch']\n",
    "    print(f\"✓ Loaded best model (epoch {best_epoch})\")\n",
    "    print(f\"  Val F1 Micro: {history['val_f1_micro'][best_epoch-1]:.4f}\")\n",
    "    print(f\"  Val F1 Macro: {history['val_f1_macro'][best_epoch-1]:.4f}\")\n",
    "else:\n",
    "    print(\"✓ Loaded model from legal_longformer_best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43888b6",
   "metadata": {},
   "source": [
    "## 3. Preprocess Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e905002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess BOTH val and test texts (uses cached summaries)\n",
    "print(\"Preprocessing validation texts (for threshold optimization)...\")\n",
    "val_texts_processed = hybrid_classifier.preprocess_texts(data.val_texts)\n",
    "\n",
    "print(\"\\nPreprocessing test texts (for final evaluation)...\")\n",
    "test_texts_processed = hybrid_classifier.preprocess_texts(data.test_texts)\n",
    "\n",
    "stats = hybrid_classifier.get_processing_stats()\n",
    "print(f\"\\nTotal processed: {stats['total_processed']}\")\n",
    "print(f\"Direct: {stats['direct_classified']}\")\n",
    "print(f\"Summarized: {stats['summarized_first']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bee31b6",
   "metadata": {},
   "source": [
    "## 4. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb646e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions and probabilities for BOTH sets\n",
    "print(\"Generating predictions on validation set...\")\n",
    "y_proba_val = hybrid_classifier.predict_proba(val_texts_processed, preprocess=False, batch_size=16)\n",
    "y_pred_val = (y_proba_val >= 0.5).astype(int)\n",
    "\n",
    "print(\"Generating predictions on test set...\")\n",
    "y_proba_test = hybrid_classifier.predict_proba(test_texts_processed, preprocess=False, batch_size=16)\n",
    "y_pred_test = (y_proba_test >= 0.5).astype(int)\n",
    "\n",
    "print(f\"\\nVal predictions shape: {y_pred_val.shape}\")\n",
    "print(f\"Test predictions shape: {y_pred_test.shape}\")\n",
    "print(f\"Positive predictions per sample (test): {y_pred_test.sum(axis=1).mean():.2f} avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fa50c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Legal-Longformer predictions for ensemble analysis\n",
    "np.savez_compressed(\n",
    "    '../outputs/legal_longformer_test_predictions.npz',\n",
    "    y_pred=y_pred_test,\n",
    "    y_proba=y_proba_test\n",
    ")\n",
    "print(\"✓ Legal-Longformer predictions saved to outputs/legal_longformer_test_predictions.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd99f09e",
   "metadata": {},
   "source": [
    "## 5. Overall Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99e2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on TEST set (held-out, final evaluation)\n",
    "evaluator = MultiLabelEvaluator(data.label_names)\n",
    "results = evaluator.evaluate(data.y_test, y_pred_test)\n",
    "\n",
    "print(\"TEST SET EVALUATION (threshold=0.5):\")\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10a2dea",
   "metadata": {},
   "source": [
    "## 6. Per-Class Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c1d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top performing classes (on TEST set, threshold=0.5)\n",
    "print(\"TOP 10 CLASSES (by F1) - TEST SET:\")\n",
    "results.get_top_classes(10, 'f1')[['label', 'precision', 'recall', 'f1', 'support']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d78c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottom performing classes\n",
    "print(\"BOTTOM 10 CLASSES (by F1):\")\n",
    "results.get_bottom_classes(10, 'f1')[['label', 'precision', 'recall', 'f1', 'support']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a7d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full per-class metrics (TEST set, threshold=0.5)\n",
    "per_class = results.per_class_metrics.copy()\n",
    "per_class = per_class.sort_values('f1', ascending=False)\n",
    "per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3a1099",
   "metadata": {},
   "source": [
    "## 7. F1 Distribution by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc073cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 distribution histogram\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "ax1 = axes[0]\n",
    "ax1.hist(per_class['f1'], bins=20, edgecolor='white', alpha=0.7)\n",
    "ax1.axvline(x=0.63, color='orange', linestyle='--', label='Human κ low (0.63)')\n",
    "ax1.axvline(x=0.93, color='green', linestyle='--', label='Human κ high (0.93)')\n",
    "median_f1 = float(per_class['f1'].median())\n",
    "ax1.axvline(x=median_f1, color='red', linestyle='-', label=f'Median: {median_f1:.2f}')\n",
    "ax1.set_xlabel('F1 Score')\n",
    "ax1.set_ylabel('Number of Classes')\n",
    "ax1.set_title('F1 Score Distribution Across Classes')\n",
    "ax1.legend()\n",
    "\n",
    "# Bar chart by class\n",
    "ax2 = axes[1]\n",
    "colors = ['green' if f1 >= 0.63 else 'orange' if f1 >= 0.5 else 'red' for f1 in per_class['f1']]\n",
    "ax2.bar(range(len(per_class)), per_class['f1'], color=colors, alpha=0.7)\n",
    "ax2.axhline(y=0.63, color='orange', linestyle='--', alpha=0.7)\n",
    "ax2.axhline(y=0.93, color='green', linestyle='--', alpha=0.7)\n",
    "ax2.set_xlabel('Class Index (sorted by F1)')\n",
    "ax2.set_ylabel('F1 Score')\n",
    "ax2.set_title('F1 by Class (green=automatable, orange=review, red=human)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3dfc4",
   "metadata": {},
   "source": [
    "## 8. Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20211004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different global thresholds on VALIDATION set\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "thresh_results = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred_t = (y_proba_val >= thresh).astype(int)\n",
    "    thresh_results.append({\n",
    "        'threshold': thresh,\n",
    "        'f1_micro': f1_score(data.y_val, y_pred_t, average='micro', zero_division=0),\n",
    "        'f1_macro': f1_score(data.y_val, y_pred_t, average='macro', zero_division=0),\n",
    "        'precision': precision_score(data.y_val, y_pred_t, average='micro', zero_division=0),\n",
    "        'recall': recall_score(data.y_val, y_pred_t, average='micro', zero_division=0),\n",
    "        'avg_preds': y_pred_t.sum(axis=1).mean(),\n",
    "    })\n",
    "\n",
    "thresh_df = pd.DataFrame(thresh_results)\n",
    "print(\"Global threshold analysis (on VALIDATION set):\")\n",
    "thresh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e86e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot threshold analysis\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(thresh_df['threshold'], thresh_df['f1_micro'], 'o-', label='F1 Micro', linewidth=2)\n",
    "ax.plot(thresh_df['threshold'], thresh_df['f1_macro'], 's-', label='F1 Macro', linewidth=2)\n",
    "ax.plot(thresh_df['threshold'], thresh_df['precision'], '^--', label='Precision', alpha=0.7)\n",
    "ax.plot(thresh_df['threshold'], thresh_df['recall'], 'v--', label='Recall', alpha=0.7)\n",
    "\n",
    "best_thresh = thresh_df.loc[thresh_df['f1_micro'].idxmax(), 'threshold']\n",
    "ax.axvline(x=float(best_thresh), color='red', linestyle=':', label=f'Best threshold: {best_thresh}') # type: ignore\n",
    "\n",
    "ax.set_xlabel('Threshold')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Threshold Analysis')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest threshold: {best_thresh} → F1 Micro: {thresh_df.loc[thresh_df['f1_micro'].idxmax(), 'f1_micro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa37c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with optimal threshold (0.6)\n",
    "OPTIMAL_THRESHOLD = 0.6\n",
    "\n",
    "y_pred_optimal = (y_proba_test >= OPTIMAL_THRESHOLD).astype(int)\n",
    "results_optimal = evaluator.evaluate(data.y_test, y_pred_optimal)\n",
    "\n",
    "print(f\"=== Results with Optimal Threshold ({OPTIMAL_THRESHOLD}) ===\")\n",
    "print(results_optimal.summary())\n",
    "\n",
    "print(f\"\\nImprovement over default (0.5):\")\n",
    "print(f\"  F1 Micro: {results.f1_micro:.4f} → {results_optimal.f1_micro:.4f} ({(results_optimal.f1_micro - results.f1_micro)*100:+.1f}%)\")\n",
    "print(f\"  F1 Macro: {results.f1_macro:.4f} → {results_optimal.f1_macro:.4f} ({(results_optimal.f1_macro - results.f1_macro)*100:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4924e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class performance with optimal threshold\n",
    "per_class_optimal = results_optimal.per_class_metrics\n",
    "\n",
    "print(f\"TOP 10 CLASSES (threshold={OPTIMAL_THRESHOLD}):\")\n",
    "display(per_class_optimal.nlargest(10, 'f1')[['label', 'precision', 'recall', 'f1', 'support']])\n",
    "\n",
    "print(f\"\\nBOTTOM 10 CLASSES (threshold={OPTIMAL_THRESHOLD}):\")\n",
    "display(per_class_optimal.nsmallest(10, 'f1')[['label', 'precision', 'recall', 'f1', 'support']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13df7075",
   "metadata": {},
   "source": [
    "## 9. Prediction Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc156a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze probability distribution (TEST set)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Overall probability distribution\n",
    "ax1 = axes[0]\n",
    "ax1.hist(y_proba_test.flatten(), bins=50, edgecolor='white', alpha=0.7)\n",
    "ax1.axvline(x=0.5, color='red', linestyle='--', label='Threshold (0.5)')\n",
    "ax1.set_xlabel('Predicted Probability')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Overall Probability Distribution (Test Set)')\n",
    "ax1.legend()\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Predictions per sample\n",
    "ax2 = axes[1]\n",
    "preds_per_sample = y_pred_test.sum(axis=1)\n",
    "max_preds = int(preds_per_sample.max())\n",
    "mean_preds = float(preds_per_sample.mean())\n",
    "ax2.hist(preds_per_sample, bins=range(0, max_preds + 2), edgecolor='white', alpha=0.7)\n",
    "ax2.axvline(x=mean_preds, color='red', linestyle='--', label=f'Mean: {mean_preds:.2f}')\n",
    "ax2.set_xlabel('Number of Predictions per Sample')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Predictions per Sample Distribution')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Samples with 0 predictions: {(preds_per_sample == 0).sum()} ({(preds_per_sample == 0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9825f72d",
   "metadata": {},
   "source": [
    "## 10. Feasibility Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94606250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feasibility vs human annotator agreement\n",
    "feasibility = results.get_feasibility_analysis(human_kappa_low=0.63, human_kappa_high=0.93)\n",
    "\n",
    "print(\"=== AUTOMATION FEASIBILITY ===\")\n",
    "print(f\"Fully automatable (F1 ≥ 0.63):  {feasibility['automation_feasible'].sum()} / {len(feasibility)}\")\n",
    "print(f\"High confidence (F1 ≥ 0.93):    {feasibility['high_confidence'].sum()} / {len(feasibility)}\")\n",
    "print(f\"Needs review (0.50 ≤ F1 < 0.63): {feasibility['needs_review'].sum()} / {len(feasibility)}\")\n",
    "print(f\"Not feasible (F1 < 0.50):       {(~feasibility['automation_feasible'] & ~feasibility['needs_review']).sum()} / {len(feasibility)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3421504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show automatable classes\n",
    "print(\"\\nCLASSES FEASIBLE FOR AUTOMATION:\")\n",
    "feasibility[feasibility['automation_feasible']][['label', 'f1', 'support']].sort_values('f1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ef254c",
   "metadata": {},
   "source": [
    "## 10. Per-Class Threshold Optimization\n",
    "\n",
    "Find the optimal threshold for each class independently to maximize F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3dbe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal threshold for each class using VALIDATION set\n",
    "# Thresholds will be applied to TEST set for unbiased final evaluation\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "thresholds_to_try = np.arange(0.1, 0.95, 0.05)\n",
    "optimal_thresholds = []\n",
    "\n",
    "for class_idx in range(len(data.label_names)):\n",
    "    best_f1 = 0\n",
    "    best_thresh = 0.5\n",
    "    \n",
    "    # Use VALIDATION set for optimization\n",
    "    y_true_class = data.y_val[:, class_idx]\n",
    "    y_proba_class = y_proba_val[:, class_idx]\n",
    "    \n",
    "    # Calculate F1 at default threshold (0.5)\n",
    "    y_pred_at_05 = (y_proba_class >= 0.5).astype(int)\n",
    "    if y_pred_at_05.sum() > 0 and y_pred_at_05.sum() < len(y_pred_at_05):\n",
    "        f1_at_05 = f1_score(y_true_class, y_pred_at_05, zero_division=0)\n",
    "    else:\n",
    "        f1_at_05 = 0\n",
    "    \n",
    "    for t in thresholds_to_try:\n",
    "        y_pred_class = (y_proba_class >= t).astype(int)\n",
    "        # Handle case where all predictions are 0 or all are 1\n",
    "        if y_pred_class.sum() == 0 or y_pred_class.sum() == len(y_pred_class):\n",
    "            continue\n",
    "        f1 = f1_score(y_true_class, y_pred_class, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thresh = t\n",
    "    \n",
    "    optimal_thresholds.append({\n",
    "        'class_idx': class_idx,\n",
    "        'label': data.label_names[class_idx],\n",
    "        'optimal_threshold': best_thresh,\n",
    "        'f1_at_0.5': f1_at_05,\n",
    "        'f1_at_optimal': best_f1,\n",
    "        'improvement': best_f1 - f1_at_05,\n",
    "        'support': int(y_true_class.sum())\n",
    "    })\n",
    "\n",
    "threshold_df = pd.DataFrame(optimal_thresholds)\n",
    "print(f\"Per-class threshold optimization (on VALIDATION set):\")\n",
    "print(f\"  Threshold range: {threshold_df['optimal_threshold'].min():.2f} - {threshold_df['optimal_threshold'].max():.2f}\")\n",
    "print(f\"  Mean threshold:  {threshold_df['optimal_threshold'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb688b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show classes with biggest improvements\n",
    "print(\"TOP 10 CLASSES BY IMPROVEMENT:\")\n",
    "display(threshold_df.nlargest(10, 'improvement')[\n",
    "    ['label', 'optimal_threshold', 'f1_at_0.5', 'f1_at_optimal', 'improvement', 'support']\n",
    "])\n",
    "\n",
    "# Threshold distribution\n",
    "print(f\"\\nThreshold distribution:\")\n",
    "print(threshold_df['optimal_threshold'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0943fda6",
   "metadata": {},
   "outputs": [],
   "source": "# Apply per-class thresholds (from VAL) to TEST set for final unbiased evaluation\nper_class_thresholds = threshold_df['optimal_threshold'].values\n\n# Apply to TEST predictions\ny_pred_test_perclass = np.zeros_like(y_proba_test, dtype=int)\nfor class_idx in range(len(data.label_names)):\n    y_pred_test_perclass[:, class_idx] = (y_proba_test[:, class_idx] >= per_class_thresholds[class_idx]).astype(int)\n\n# Also apply global optimal threshold (0.6) to TEST\nOPTIMAL_THRESHOLD = 0.6\ny_pred_test_optimal = (y_proba_test >= OPTIMAL_THRESHOLD).astype(int)\n\n# Evaluate all configurations on TEST set\nresults_test_05 = evaluator.evaluate(data.y_test, y_pred_test)\nresults_test_06 = evaluator.evaluate(data.y_test, y_pred_test_optimal)\nresults_test_perclass = evaluator.evaluate(data.y_test, y_pred_test_perclass)\n\n# Load TF-IDF predictions and compute metrics dynamically (instead of hardcoding)\ntfidf_pred_path = Path('../outputs/tfidf_test_predictions.npz')\nif tfidf_pred_path.exists():\n    tfidf_data = np.load(tfidf_pred_path)\n    y_pred_tfidf = tfidf_data['y_pred']\n    results_tfidf = evaluator.evaluate(data.y_test, y_pred_tfidf)\n    TFIDF_F1_MICRO = results_tfidf.f1_micro\n    TFIDF_F1_MACRO = results_tfidf.f1_macro\n    TFIDF_PRECISION = results_tfidf.precision_micro\n    TFIDF_RECALL = results_tfidf.recall_micro\nelse:\n    raise FileNotFoundError(f\"Run NB 02 first to generate {tfidf_pred_path}\")\n\nprint(\"=\" * 80)\nprint(\"FINAL EVALUATION ON TEST SET (thresholds optimized on validation)\")\nprint(\"=\" * 80)\nprint(f\"{'Metric':<15} {'TF-IDF':<12} {'LF (0.5)':<12} {'LF (0.6)':<12} {'LF Per-Class':<12}\")\nprint(\"-\" * 80)\nprint(f\"{'F1 Micro':<15} {TFIDF_F1_MICRO:<12.4f} {results_test_05.f1_micro:<12.4f} {results_test_06.f1_micro:<12.4f} {results_test_perclass.f1_micro:<12.4f}\")\nprint(f\"{'F1 Macro':<15} {TFIDF_F1_MACRO:<12.4f} {results_test_05.f1_macro:<12.4f} {results_test_06.f1_macro:<12.4f} {results_test_perclass.f1_macro:<12.4f}\")\nprint(f\"{'Precision':<15} {TFIDF_PRECISION:<12.4f} {results_test_05.precision_micro:<12.4f} {results_test_06.precision_micro:<12.4f} {results_test_perclass.precision_micro:<12.4f}\")\nprint(f\"{'Recall':<15} {TFIDF_RECALL:<12.4f} {results_test_05.recall_micro:<12.4f} {results_test_06.recall_micro:<12.4f} {results_test_perclass.recall_micro:<12.4f}\")\nprint(\"=\" * 80)\nprint(f\"\\nGap to TF-IDF (F1 Micro): {(results_test_perclass.f1_micro - TFIDF_F1_MICRO)*100:+.1f}%\")\nprint(f\"Gap to TF-IDF (F1 Macro): {(results_test_perclass.f1_macro - TFIDF_F1_MACRO)*100:+.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "id": "8689c00e",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92251095",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "if history:\n",
    "    print(f\"\\nModel: Legal-Longformer (epoch {history['best_epoch']})\")\n",
    "    print(f\"Validation F1 Micro: {history['best_f1']:.4f}\")\n",
    "    print(f\"Validation F1 Macro: {history['val_f1_macro'][history['best_epoch']-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nTest Set Performance (threshold=0.5):\")\n",
    "print(f\"  F1 Micro:      {results.f1_micro:.4f}\")\n",
    "print(f\"  F1 Macro:      {results.f1_macro:.4f}\")\n",
    "print(f\"  F1 Weighted:   {results.f1_weighted:.4f}\")\n",
    "print(f\"  Precision:     {results.precision_micro:.4f}\")\n",
    "print(f\"  Recall:        {results.recall_micro:.4f}\")\n",
    "\n",
    "print(f\"\\nOptimal Threshold Analysis:\")\n",
    "best_idx = thresh_df['f1_micro'].idxmax()\n",
    "print(f\"  Best threshold:  {thresh_df.loc[best_idx, 'threshold']}\")\n",
    "print(f\"  F1 Micro:        {thresh_df.loc[best_idx, 'f1_micro']:.4f}\")\n",
    "\n",
    "print(f\"\\nAutomation Feasibility (vs human κ=0.63-0.93):\")\n",
    "print(f\"  High confidence (F1≥0.93): {feasibility['high_confidence'].sum()} / {len(feasibility)} classes\")\n",
    "print(f\"  Automatable (F1≥0.63):     {feasibility['automation_feasible'].sum()} / {len(feasibility)} classes\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db34ed37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c2e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Legal-Longformer predictions for ensemble analysis\n",
    "from src.model_evaluator import save_predictions\n",
    "\n",
    "pred_path = Path('../outputs/longformer_predictions.npz')\n",
    "save_predictions(y_pred_test, y_proba_test, str(pred_path))\n",
    "print(f\"Saved Legal-Longformer predictions to {pred_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tr_challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}